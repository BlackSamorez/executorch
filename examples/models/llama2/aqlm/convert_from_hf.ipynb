{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.weight: value.shape=torch.Size([128256, 4096])\n",
      "tok_embeddings.weight: value.shape=torch.Size([128256, 4096])\n",
      "layers.0.attention_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.0.feed_forward.w2.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.0.feed_forward.w2.codes: value.shape=torch.Size([1792, 4096, 2])\n",
      "layers.0.feed_forward.w2.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.0.feed_forward.w1.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.0.feed_forward.w1.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.0.feed_forward.w1.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.0.feed_forward.w3.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.0.feed_forward.w3.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.0.feed_forward.w3.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.0.ffn_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.0.attention.wk.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.0.attention.wk.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.0.attention.wk.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.0.attention.wo.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.0.attention.wo.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.0.attention.wo.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.0.attention.wq.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.0.attention.wq.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.0.attention.wq.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.0.attention.wv.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.0.attention.wv.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.0.attention.wv.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.1.attention_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.1.feed_forward.w2.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.1.feed_forward.w2.codes: value.shape=torch.Size([1792, 4096, 2])\n",
      "layers.1.feed_forward.w2.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.1.feed_forward.w1.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.1.feed_forward.w1.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.1.feed_forward.w1.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.1.feed_forward.w3.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.1.feed_forward.w3.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.1.feed_forward.w3.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.1.ffn_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.1.attention.wk.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.1.attention.wk.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.1.attention.wk.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.1.attention.wo.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.1.attention.wo.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.1.attention.wo.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.1.attention.wq.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.1.attention.wq.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.1.attention.wq.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.1.attention.wv.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.1.attention.wv.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.1.attention.wv.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.10.attention_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.10.feed_forward.w2.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.10.feed_forward.w2.codes: value.shape=torch.Size([1792, 4096, 2])\n",
      "layers.10.feed_forward.w2.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.10.feed_forward.w1.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.10.feed_forward.w1.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.10.feed_forward.w1.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.10.feed_forward.w3.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.10.feed_forward.w3.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.10.feed_forward.w3.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.10.ffn_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.10.attention.wk.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.10.attention.wk.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.10.attention.wk.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.10.attention.wo.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.10.attention.wo.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.10.attention.wo.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.10.attention.wq.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.10.attention.wq.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.10.attention.wq.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.10.attention.wv.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.10.attention.wv.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.10.attention.wv.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.11.attention_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.11.feed_forward.w2.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.11.feed_forward.w2.codes: value.shape=torch.Size([1792, 4096, 2])\n",
      "layers.11.feed_forward.w2.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.11.feed_forward.w1.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.11.feed_forward.w1.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.11.feed_forward.w1.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.11.feed_forward.w3.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.11.feed_forward.w3.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.11.feed_forward.w3.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.11.ffn_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.11.attention.wk.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.11.attention.wk.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.11.attention.wk.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.11.attention.wo.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.11.attention.wo.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.11.attention.wo.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.11.attention.wq.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.11.attention.wq.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.11.attention.wq.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.11.attention.wv.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.11.attention.wv.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.11.attention.wv.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.12.attention_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.12.feed_forward.w2.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.12.feed_forward.w2.codes: value.shape=torch.Size([1792, 4096, 2])\n",
      "layers.12.feed_forward.w2.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.12.feed_forward.w1.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.12.feed_forward.w1.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.12.feed_forward.w1.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.12.feed_forward.w3.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.12.feed_forward.w3.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.12.feed_forward.w3.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.12.ffn_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.12.attention.wk.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.12.attention.wk.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.12.attention.wk.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.12.attention.wo.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.12.attention.wo.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.12.attention.wo.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.12.attention.wq.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.12.attention.wq.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.12.attention.wq.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.12.attention.wv.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.12.attention.wv.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.12.attention.wv.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.13.attention_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.13.feed_forward.w2.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.13.feed_forward.w2.codes: value.shape=torch.Size([1792, 4096, 2])\n",
      "layers.13.feed_forward.w2.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.13.feed_forward.w1.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.13.feed_forward.w1.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.13.feed_forward.w1.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.13.feed_forward.w3.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.13.feed_forward.w3.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.13.feed_forward.w3.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.13.ffn_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.13.attention.wk.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.13.attention.wk.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.13.attention.wk.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.13.attention.wo.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.13.attention.wo.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.13.attention.wo.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.13.attention.wq.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.13.attention.wq.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.13.attention.wq.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.13.attention.wv.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.13.attention.wv.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.13.attention.wv.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.14.attention_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.14.feed_forward.w2.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.14.feed_forward.w2.codes: value.shape=torch.Size([1792, 4096, 2])\n",
      "layers.14.feed_forward.w2.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.14.feed_forward.w1.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.14.feed_forward.w1.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.14.feed_forward.w1.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.14.feed_forward.w3.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.14.feed_forward.w3.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.14.feed_forward.w3.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.14.ffn_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.14.attention.wk.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.14.attention.wk.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.14.attention.wk.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.14.attention.wo.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.14.attention.wo.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.14.attention.wo.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.14.attention.wq.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.14.attention.wq.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.14.attention.wq.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.14.attention.wv.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.14.attention.wv.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.14.attention.wv.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.15.attention_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.15.feed_forward.w2.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.15.feed_forward.w2.codes: value.shape=torch.Size([1792, 4096, 2])\n",
      "layers.15.feed_forward.w2.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.15.feed_forward.w1.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.15.feed_forward.w1.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.15.feed_forward.w1.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.15.feed_forward.w3.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.15.feed_forward.w3.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.15.feed_forward.w3.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.15.ffn_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.15.attention.wk.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.15.attention.wk.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.15.attention.wk.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.15.attention.wo.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.15.attention.wo.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.15.attention.wo.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.15.attention.wq.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.15.attention.wq.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.15.attention.wq.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.15.attention.wv.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.15.attention.wv.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.15.attention.wv.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.16.attention_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.16.feed_forward.w2.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.16.feed_forward.w2.codes: value.shape=torch.Size([1792, 4096, 2])\n",
      "layers.16.feed_forward.w2.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.16.feed_forward.w1.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.16.feed_forward.w1.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.16.feed_forward.w1.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.16.feed_forward.w3.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.16.feed_forward.w3.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.16.feed_forward.w3.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.16.ffn_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.16.attention.wk.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.16.attention.wk.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.16.attention.wk.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.16.attention.wo.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.16.attention.wo.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.16.attention.wo.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.16.attention.wq.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.16.attention.wq.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.16.attention.wq.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.16.attention.wv.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.16.attention.wv.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.16.attention.wv.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.17.attention_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.17.feed_forward.w2.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.17.feed_forward.w2.codes: value.shape=torch.Size([1792, 4096, 2])\n",
      "layers.17.feed_forward.w2.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.17.feed_forward.w1.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.17.feed_forward.w1.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.17.feed_forward.w1.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.17.feed_forward.w3.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.17.feed_forward.w3.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.17.feed_forward.w3.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.17.ffn_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.17.attention.wk.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.17.attention.wk.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.17.attention.wk.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.17.attention.wo.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.17.attention.wo.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.17.attention.wo.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.17.attention.wq.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.17.attention.wq.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.17.attention.wq.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.17.attention.wv.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.17.attention.wv.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.17.attention.wv.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.18.attention_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.18.feed_forward.w2.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.18.feed_forward.w2.codes: value.shape=torch.Size([1792, 4096, 2])\n",
      "layers.18.feed_forward.w2.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.18.feed_forward.w1.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.18.feed_forward.w1.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.18.feed_forward.w1.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.18.feed_forward.w3.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.18.feed_forward.w3.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.18.feed_forward.w3.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.18.ffn_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.18.attention.wk.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.18.attention.wk.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.18.attention.wk.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.18.attention.wo.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.18.attention.wo.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.18.attention.wo.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.18.attention.wq.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.18.attention.wq.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.18.attention.wq.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.18.attention.wv.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.18.attention.wv.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.18.attention.wv.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.19.attention_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.19.feed_forward.w2.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.19.feed_forward.w2.codes: value.shape=torch.Size([1792, 4096, 2])\n",
      "layers.19.feed_forward.w2.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.19.feed_forward.w1.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.19.feed_forward.w1.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.19.feed_forward.w1.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.19.feed_forward.w3.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.19.feed_forward.w3.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.19.feed_forward.w3.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.19.ffn_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.19.attention.wk.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.19.attention.wk.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.19.attention.wk.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.19.attention.wo.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.19.attention.wo.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.19.attention.wo.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.19.attention.wq.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.19.attention.wq.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.19.attention.wq.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.19.attention.wv.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.19.attention.wv.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.19.attention.wv.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.2.attention_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.2.feed_forward.w2.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.2.feed_forward.w2.codes: value.shape=torch.Size([1792, 4096, 2])\n",
      "layers.2.feed_forward.w2.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.2.feed_forward.w1.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.2.feed_forward.w1.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.2.feed_forward.w1.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.2.feed_forward.w3.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.2.feed_forward.w3.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.2.feed_forward.w3.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.2.ffn_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.2.attention.wk.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.2.attention.wk.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.2.attention.wk.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.2.attention.wo.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.2.attention.wo.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.2.attention.wo.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.2.attention.wq.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.2.attention.wq.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.2.attention.wq.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.2.attention.wv.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.2.attention.wv.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.2.attention.wv.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.20.attention_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.20.feed_forward.w2.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.20.feed_forward.w2.codes: value.shape=torch.Size([1792, 4096, 2])\n",
      "layers.20.feed_forward.w2.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.20.feed_forward.w1.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.20.feed_forward.w1.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.20.feed_forward.w1.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.20.feed_forward.w3.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.20.feed_forward.w3.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.20.feed_forward.w3.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.20.ffn_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.20.attention.wk.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.20.attention.wk.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.20.attention.wk.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.20.attention.wo.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.20.attention.wo.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.20.attention.wo.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.20.attention.wq.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.20.attention.wq.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.20.attention.wq.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.20.attention.wv.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.20.attention.wv.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.20.attention.wv.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.21.attention_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.21.feed_forward.w2.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.21.feed_forward.w2.codes: value.shape=torch.Size([1792, 4096, 2])\n",
      "layers.21.feed_forward.w2.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.21.feed_forward.w1.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.21.feed_forward.w1.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.21.feed_forward.w1.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.21.feed_forward.w3.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.21.feed_forward.w3.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.21.feed_forward.w3.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.21.ffn_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.21.attention.wk.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.21.attention.wk.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.21.attention.wk.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.21.attention.wo.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.21.attention.wo.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.21.attention.wo.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.21.attention.wq.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.21.attention.wq.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.21.attention.wq.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.21.attention.wv.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.21.attention.wv.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.21.attention.wv.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.22.attention_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.22.feed_forward.w2.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.22.feed_forward.w2.codes: value.shape=torch.Size([1792, 4096, 2])\n",
      "layers.22.feed_forward.w2.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.22.feed_forward.w1.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.22.feed_forward.w1.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.22.feed_forward.w1.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.22.feed_forward.w3.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.22.feed_forward.w3.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.22.feed_forward.w3.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.22.ffn_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.22.attention.wk.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.22.attention.wk.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.22.attention.wk.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.22.attention.wo.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.22.attention.wo.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.22.attention.wo.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.22.attention.wq.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.22.attention.wq.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.22.attention.wq.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.22.attention.wv.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.22.attention.wv.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.22.attention.wv.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.23.attention_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.23.feed_forward.w2.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.23.feed_forward.w2.codes: value.shape=torch.Size([1792, 4096, 2])\n",
      "layers.23.feed_forward.w2.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.23.feed_forward.w1.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.23.feed_forward.w1.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.23.feed_forward.w1.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.23.feed_forward.w3.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.23.feed_forward.w3.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.23.feed_forward.w3.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.23.ffn_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.23.attention.wk.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.23.attention.wk.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.23.attention.wk.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.23.attention.wo.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.23.attention.wo.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.23.attention.wo.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.23.attention.wq.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.23.attention.wq.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.23.attention.wq.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.23.attention.wv.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.23.attention.wv.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.23.attention.wv.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.24.attention_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.24.feed_forward.w2.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.24.feed_forward.w2.codes: value.shape=torch.Size([1792, 4096, 2])\n",
      "layers.24.feed_forward.w2.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.24.feed_forward.w1.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.24.feed_forward.w1.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.24.feed_forward.w1.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.24.feed_forward.w3.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.24.feed_forward.w3.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.24.feed_forward.w3.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.24.ffn_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.24.attention.wk.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.24.attention.wk.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.24.attention.wk.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.24.attention.wo.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.24.attention.wo.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.24.attention.wo.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.24.attention.wq.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.24.attention.wq.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.24.attention.wq.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.24.attention.wv.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.24.attention.wv.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.24.attention.wv.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.25.attention_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.25.feed_forward.w2.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.25.feed_forward.w2.codes: value.shape=torch.Size([1792, 4096, 2])\n",
      "layers.25.feed_forward.w2.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.25.feed_forward.w1.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.25.feed_forward.w1.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.25.feed_forward.w1.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.25.feed_forward.w3.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.25.feed_forward.w3.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.25.feed_forward.w3.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.25.ffn_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.25.attention.wk.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.25.attention.wk.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.25.attention.wk.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.25.attention.wo.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.25.attention.wo.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.25.attention.wo.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.25.attention.wq.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.25.attention.wq.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.25.attention.wq.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.25.attention.wv.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.25.attention.wv.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.25.attention.wv.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.26.attention_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.26.feed_forward.w2.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.26.feed_forward.w2.codes: value.shape=torch.Size([1792, 4096, 2])\n",
      "layers.26.feed_forward.w2.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.26.feed_forward.w1.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.26.feed_forward.w1.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.26.feed_forward.w1.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.26.feed_forward.w3.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.26.feed_forward.w3.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.26.feed_forward.w3.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.26.ffn_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.26.attention.wk.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.26.attention.wk.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.26.attention.wk.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.26.attention.wo.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.26.attention.wo.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.26.attention.wo.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.26.attention.wq.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.26.attention.wq.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.26.attention.wq.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.26.attention.wv.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.26.attention.wv.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.26.attention.wv.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.27.attention_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.27.feed_forward.w2.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.27.feed_forward.w2.codes: value.shape=torch.Size([1792, 4096, 2])\n",
      "layers.27.feed_forward.w2.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.27.feed_forward.w1.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.27.feed_forward.w1.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.27.feed_forward.w1.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.27.feed_forward.w3.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.27.feed_forward.w3.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.27.feed_forward.w3.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.27.ffn_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.27.attention.wk.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.27.attention.wk.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.27.attention.wk.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.27.attention.wo.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.27.attention.wo.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.27.attention.wo.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.27.attention.wq.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.27.attention.wq.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.27.attention.wq.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.27.attention.wv.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.27.attention.wv.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.27.attention.wv.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.28.attention_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.28.feed_forward.w2.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.28.feed_forward.w2.codes: value.shape=torch.Size([1792, 4096, 2])\n",
      "layers.28.feed_forward.w2.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.28.feed_forward.w1.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.28.feed_forward.w1.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.28.feed_forward.w1.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.28.feed_forward.w3.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.28.feed_forward.w3.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.28.feed_forward.w3.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.28.ffn_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.28.attention.wk.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.28.attention.wk.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.28.attention.wk.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.28.attention.wo.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.28.attention.wo.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.28.attention.wo.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.28.attention.wq.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.28.attention.wq.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.28.attention.wq.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.28.attention.wv.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.28.attention.wv.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.28.attention.wv.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.29.attention_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.29.feed_forward.w2.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.29.feed_forward.w2.codes: value.shape=torch.Size([1792, 4096, 2])\n",
      "layers.29.feed_forward.w2.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.29.feed_forward.w1.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.29.feed_forward.w1.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.29.feed_forward.w1.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.29.feed_forward.w3.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.29.feed_forward.w3.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.29.feed_forward.w3.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.29.ffn_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.29.attention.wk.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.29.attention.wk.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.29.attention.wk.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.29.attention.wo.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.29.attention.wo.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.29.attention.wo.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.29.attention.wq.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.29.attention.wq.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.29.attention.wq.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.29.attention.wv.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.29.attention.wv.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.29.attention.wv.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.3.attention_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.3.feed_forward.w2.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.3.feed_forward.w2.codes: value.shape=torch.Size([1792, 4096, 2])\n",
      "layers.3.feed_forward.w2.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.3.feed_forward.w1.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.3.feed_forward.w1.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.3.feed_forward.w1.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.3.feed_forward.w3.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.3.feed_forward.w3.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.3.feed_forward.w3.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.3.ffn_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.3.attention.wk.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.3.attention.wk.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.3.attention.wk.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.3.attention.wo.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.3.attention.wo.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.3.attention.wo.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.3.attention.wq.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.3.attention.wq.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.3.attention.wq.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.3.attention.wv.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.3.attention.wv.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.3.attention.wv.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.30.attention_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.30.feed_forward.w2.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.30.feed_forward.w2.codes: value.shape=torch.Size([1792, 4096, 2])\n",
      "layers.30.feed_forward.w2.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.30.feed_forward.w1.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.30.feed_forward.w1.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.30.feed_forward.w1.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.30.feed_forward.w3.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.30.feed_forward.w3.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.30.feed_forward.w3.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.30.ffn_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.30.attention.wk.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.30.attention.wk.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.30.attention.wk.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.30.attention.wo.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.30.attention.wo.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.30.attention.wo.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.30.attention.wq.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.30.attention.wq.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.30.attention.wq.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.30.attention.wv.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.30.attention.wv.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.30.attention.wv.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.31.attention_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.31.feed_forward.w2.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.31.feed_forward.w2.codes: value.shape=torch.Size([1792, 4096, 2])\n",
      "layers.31.feed_forward.w2.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.31.feed_forward.w1.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.31.feed_forward.w1.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.31.feed_forward.w1.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.31.feed_forward.w3.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.31.feed_forward.w3.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.31.feed_forward.w3.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.31.ffn_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.31.attention.wk.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.31.attention.wk.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.31.attention.wk.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.31.attention.wo.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.31.attention.wo.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.31.attention.wo.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.31.attention.wq.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.31.attention.wq.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.31.attention.wq.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.31.attention.wv.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.31.attention.wv.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.31.attention.wv.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.4.attention_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.4.feed_forward.w2.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.4.feed_forward.w2.codes: value.shape=torch.Size([1792, 4096, 2])\n",
      "layers.4.feed_forward.w2.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.4.feed_forward.w1.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.4.feed_forward.w1.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.4.feed_forward.w1.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.4.feed_forward.w3.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.4.feed_forward.w3.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.4.feed_forward.w3.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.4.ffn_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.4.attention.wk.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.4.attention.wk.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.4.attention.wk.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.4.attention.wo.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.4.attention.wo.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.4.attention.wo.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.4.attention.wq.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.4.attention.wq.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.4.attention.wq.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.4.attention.wv.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.4.attention.wv.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.4.attention.wv.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.5.attention_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.5.feed_forward.w2.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.5.feed_forward.w2.codes: value.shape=torch.Size([1792, 4096, 2])\n",
      "layers.5.feed_forward.w2.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.5.feed_forward.w1.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.5.feed_forward.w1.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.5.feed_forward.w1.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.5.feed_forward.w3.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.5.feed_forward.w3.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.5.feed_forward.w3.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.5.ffn_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.5.attention.wk.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.5.attention.wk.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.5.attention.wk.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.5.attention.wo.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.5.attention.wo.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.5.attention.wo.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.5.attention.wq.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.5.attention.wq.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.5.attention.wq.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.5.attention.wv.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.5.attention.wv.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.5.attention.wv.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.6.attention_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.6.feed_forward.w2.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.6.feed_forward.w2.codes: value.shape=torch.Size([1792, 4096, 2])\n",
      "layers.6.feed_forward.w2.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.6.feed_forward.w1.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.6.feed_forward.w1.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.6.feed_forward.w1.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.6.feed_forward.w3.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.6.feed_forward.w3.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.6.feed_forward.w3.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.6.ffn_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.6.attention.wk.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.6.attention.wk.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.6.attention.wk.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.6.attention.wo.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.6.attention.wo.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.6.attention.wo.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.6.attention.wq.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.6.attention.wq.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.6.attention.wq.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.6.attention.wv.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.6.attention.wv.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.6.attention.wv.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.7.attention_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.7.feed_forward.w2.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.7.feed_forward.w2.codes: value.shape=torch.Size([1792, 4096, 2])\n",
      "layers.7.feed_forward.w2.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.7.feed_forward.w1.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.7.feed_forward.w1.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.7.feed_forward.w1.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.7.feed_forward.w3.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.7.feed_forward.w3.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.7.feed_forward.w3.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.7.ffn_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.7.attention.wk.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.7.attention.wk.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.7.attention.wk.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.7.attention.wo.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.7.attention.wo.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.7.attention.wo.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.7.attention.wq.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.7.attention.wq.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.7.attention.wq.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.7.attention.wv.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.7.attention.wv.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.7.attention.wv.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.8.attention_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.8.feed_forward.w2.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.8.feed_forward.w2.codes: value.shape=torch.Size([1792, 4096, 2])\n",
      "layers.8.feed_forward.w2.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.8.feed_forward.w1.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.8.feed_forward.w1.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.8.feed_forward.w1.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.8.feed_forward.w3.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.8.feed_forward.w3.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.8.feed_forward.w3.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.8.ffn_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.8.attention.wk.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.8.attention.wk.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.8.attention.wk.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.8.attention.wo.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.8.attention.wo.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.8.attention.wo.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.8.attention.wq.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.8.attention.wq.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.8.attention.wq.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.8.attention.wv.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.8.attention.wv.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.8.attention.wv.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.9.attention_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.9.feed_forward.w2.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.9.feed_forward.w2.codes: value.shape=torch.Size([1792, 4096, 2])\n",
      "layers.9.feed_forward.w2.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.9.feed_forward.w1.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.9.feed_forward.w1.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.9.feed_forward.w1.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.9.feed_forward.w3.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.9.feed_forward.w3.codes: value.shape=torch.Size([512, 14336, 2])\n",
      "layers.9.feed_forward.w3.scales: value.shape=torch.Size([14336, 1, 1, 1])\n",
      "layers.9.ffn_norm.weight: value.shape=torch.Size([4096])\n",
      "layers.9.attention.wk.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.9.attention.wk.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.9.attention.wk.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "layers.9.attention.wo.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.9.attention.wo.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.9.attention.wo.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.9.attention.wq.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.9.attention.wq.codes: value.shape=torch.Size([512, 4096, 2])\n",
      "layers.9.attention.wq.scales: value.shape=torch.Size([4096, 1, 1, 1])\n",
      "layers.9.attention.wv.codebooks: value.shape=torch.Size([2, 256, 1, 8])\n",
      "layers.9.attention.wv.codes: value.shape=torch.Size([512, 1024, 2])\n",
      "layers.9.attention.wv.scales: value.shape=torch.Size([1024, 1, 1, 1])\n",
      "norm.weight: value.shape=torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "LOAD_PATH = \"/Users/blacksamorez/models/Meta-Llama-3.1-8B-Instruct-AQLM-PV-2Bit-2x8-hf/\"\n",
    "SAVE_PATH = \"/Users/blacksamorez/models/Meta-Llama-3.1-8B-Instruct-AQLM-PV-2Bit-2x8-hf/\"\n",
    "\n",
    "\n",
    "with open(LOAD_PATH + \"config.json\", \"r\") as file:\n",
    "    hf_config = json.load(file)\n",
    "    \n",
    "with open(LOAD_PATH + \"params.json\", \"w\") as file:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"dim\": hf_config[\"hidden_size\"],\n",
    "            \"multiple_of\": 256,\n",
    "            \"n_heads\": hf_config[\"num_attention_heads\"],\n",
    "            \"n_kv_heads\": hf_config[\"num_key_value_heads\"],\n",
    "            \"n_layers\": hf_config[\"num_hidden_layers\"],\n",
    "            \"norm_eps\": hf_config[\"rms_norm_eps\"],\n",
    "            \"vocab_size\": hf_config[\"vocab_size\"],\n",
    "            \"ffn_dim_multiplier\":  hf_config['intermediate_size'] / (8 / 3 * hf_config['hidden_size']),\n",
    "        },\n",
    "        file,\n",
    "    )\n",
    "\n",
    "hidden_size = hf_config[\"hidden_size\"]\n",
    "head_dim = hf_config[\"hidden_size\"] // hf_config[\"num_attention_heads\"]\n",
    "\n",
    "dict = load_file(LOAD_PATH + \"model.safetensors\")\n",
    "\n",
    "mapping = {\n",
    "    \"model.\": \"\",\n",
    "    \n",
    "    \"self_attn.q_proj\": \"attention.wq\",\n",
    "    \"self_attn.k_proj\": \"attention.wk\",\n",
    "    \"self_attn.v_proj\": \"attention.wv\",\n",
    "    \"self_attn.o_proj\": \"attention.wo\",\n",
    "    \n",
    "    \"mlp.up_proj\": \"feed_forward.w3\",\n",
    "    \"mlp.gate_proj\": \"feed_forward.w1\",\n",
    "    \"mlp.down_proj\": \"feed_forward.w2\",\n",
    "    \n",
    "    \"input_layernorm\": \"attention_norm\",\n",
    "    \"post_attention_layernorm\": \"ffn_norm\",\n",
    "    \n",
    "    \"lm_head\": \"output\",\n",
    "    \"embed_tokens\": \"tok_embeddings\",\n",
    "}\n",
    "\n",
    "\n",
    "new_dict = {}\n",
    "\n",
    "\n",
    "for key, value in dict.items():\n",
    "    for old, new in mapping.items():\n",
    "        key = key.replace(old, new)\n",
    "        \n",
    "    if \"attention.wq.codes\" in key or \"attention.wk.codes\" in key:\n",
    "        # [num_out_groups, num_in_groups, num_codebooks]\n",
    "        value = (value.reshape(-1, 2, head_dim // 2, hidden_size // 8, 2)\n",
    "            .transpose(1, 2)\n",
    "            .reshape(-1, hidden_size // 8, 2))\n",
    "    \n",
    "        \n",
    "    if \"attention.wq.scales\" in key or \"attention.wk.scales\" in key:\n",
    "        # [num_out_groups, 1, 1, 1]\n",
    "        value = (value.reshape(-1, 2, head_dim // 2, 1)\n",
    "            .transpose(1, 2)\n",
    "            .reshape(-1, 1, 1, 1))\n",
    "        \n",
    "    if \"codes\" in key:\n",
    "        value = value.transpose(0, 1) # <- Special memory layout for lut kernels\n",
    "    \n",
    "    if value.dtype == torch.float16:\n",
    "        value = value.float()\n",
    "    \n",
    "    print(f\"{key}: {value.shape=}\")\n",
    "    new_dict[key] = value\n",
    "\n",
    "torch.save(new_dict, SAVE_PATH + \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "executorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
